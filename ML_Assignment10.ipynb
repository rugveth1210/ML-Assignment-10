{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Assignment10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN9wf+nMlJFN8lKfF8lT6Lz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rugveth1210/ML-Assignment-10/blob/master/ML_Assignment10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-ije14bJbmH",
        "colab_type": "text"
      },
      "source": [
        "Problem Statement 1 : Say you are standing at the bottom of a staircase with  a dice. With each throw of the dice you either move down one step (if you get a 1 or 2 on the dice) or move up one step (if you get a 3, 4 or 5 on the dice). If you throw a 6 on the dice, you throw the dice again and move up the staircase by the number you get on that second throw. Note if you are on the base of the staircase you cannot move down! What is the probability that you will reach more than 60 steps after 250 throws of the dice. Change the code so that you have a function that takes as parameter, the number of throws\n",
        "Add a new parameter to the function that takes a probability distribution over all outcomes from a dice throw. For example (0.2,0.3,0.2,0.1,0.1,0.1) would suggest that the probability of getting a 1 is 0.2, 2 is 0.3 etc. How does that change the probability of reaching a step higher than 60?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGzwczIoYjkN",
        "colab_type": "code",
        "outputId": "aeffe0cd-753b-47ed-9938-493136e9a8c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def roll_dice(prob=None):\n",
        "    if not prob:\n",
        "        return np.random.choice(a=[1,2,3,4,5,6])\n",
        "    else:\n",
        "        return np.random.choice(a=[1,2,3,4,5,6],p=prob)\n",
        "           \n",
        "def simulate(n=100,prob=None):\n",
        "    reached=0\n",
        "    for _ in range(n):\n",
        "        throws=250\n",
        "        curr_pos=0\n",
        "        while throws:\n",
        "            throws-=1\n",
        "            curr_step=roll_dice(prob)\n",
        "            if curr_pos>60:\n",
        "                reached+=1\n",
        "                break\n",
        "                \n",
        "            if curr_step in {1,2}:\n",
        "                curr_pos-=1\n",
        "            elif curr_step in {3,4,5}:\n",
        "                curr_pos+=1\n",
        "            else:\n",
        "                curr_step=roll_dice(prob)\n",
        "                curr_pos+=curr_step\n",
        "    print(reached/n)\n",
        "simulate() #100 iterations in without probability weights \n",
        "simulate(prob=[0.2,0.3,0.2,0.1,0.1,0.1]) #100 iterations with probability weights\n",
        "\n",
        "simulate(1000)\n",
        "simulate(1000,[0.2,0.3,0.2,0.1,0.1,0.1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.28\n",
            "1.0\n",
            "0.345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiW7kkuBJuv8",
        "colab_type": "text"
      },
      "source": [
        "Problem Statement 2 : . Generate random data for for Multiple Linear Regression, Logistic Regression, K-mean Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLXx8PLaYld_",
        "colab_type": "code",
        "outputId": "dc3bbfb4-83a4-4288-a3f6-c883695aae84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#randomly generated data for the Multiple Linear regression \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import random\n",
        "from scipy.stats import norm\n",
        "random.seed(1)\n",
        "n_features = 4\n",
        "X = []\n",
        "for p in range(n_features):\n",
        "  X_p = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_p)\n",
        "eps = scipy.stats.norm.rvs(0, 0.25,100)\n",
        "y = 1 + (0.5 * X[0]) + eps + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3])\n",
        "data_mlr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y }\n",
        "df = pd.DataFrame(data_mlr)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Random data for the Logistic regression\n",
        "n_features_of_the_model = 4\n",
        "X = []\n",
        "for i in range(n_features_of_the_model):\n",
        "  X_i = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_i)\n",
        "a1 = (np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))/(1 + np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))))\n",
        "y1 = []\n",
        "for i in a1:\n",
        "  if (i>=0.5):\n",
        "    y1.append(1)\n",
        "  else:\n",
        "    y1.append(0)\n",
        "data_lr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y1 }\n",
        "df1 = pd.DataFrame(data_lr)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "\n",
        "#Random data for K-mean clustering\n",
        "X_a= -2 * np.random.rand(100,2)\n",
        "X_b = 1 + 2 * np.random.rand(50,2)\n",
        "X_a[50:100, :] = X_b\n",
        "plt.scatter(X_a[ : , 0], X_a[ :, 1], s = 50)\n",
        "plt.show()\n",
        "data_kmeans = {'X0': X_a[:,0],'X1':X_a[:,1]}\n",
        "df3 = pd.DataFrame(data_kmeans)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0 -0.514139  0.237483  0.358881 -0.218946  0.836997\n",
            "1  1.155113  1.050713  1.344456  0.676361  2.679250\n",
            "2  0.423188  0.070643 -0.155115  0.343298  1.404289\n",
            "3  0.325719  0.770574  0.229862  0.145812  1.830040\n",
            "4 -0.554816  1.108784 -0.297039  2.102812  1.934839\n",
            "          X0        X1        X2        X3         Y\n",
            "95  0.867404  0.809470 -0.698813 -0.211407  1.570258\n",
            "96 -0.314125  0.650321 -0.194280  1.757960  1.591702\n",
            "97  0.582491 -0.352278 -0.710077 -0.552536  0.880668\n",
            "98  0.282594  0.018263 -0.221446  0.554800  1.030568\n",
            "99 -0.845601 -0.326285  0.720403  0.926539  1.558880\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean    -0.118678    0.071489   -0.024178    0.328351    1.095683\n",
            "std      0.967370    0.944765    0.942306    0.970823    0.758786\n",
            "min     -2.526303   -2.467023   -2.755133   -2.302795   -0.941971\n",
            "25%     -0.801464   -0.432237   -0.610036   -0.297111    0.607452\n",
            "50%     -0.116328    0.089406   -0.052193    0.308323    1.040780\n",
            "75%      0.514649    0.814424    0.518259    0.886281    1.561725\n",
            "max      2.377999    2.449439    2.124104    3.089323    3.298092\n",
            "         X0        X1        X2        X3         Y\n",
            "0 -0.514139  0.237483  0.358881 -0.218946  0.836997\n",
            "1  1.155113  1.050713  1.344456  0.676361  2.679250\n",
            "2  0.423188  0.070643 -0.155115  0.343298  1.404289\n",
            "3  0.325719  0.770574  0.229862  0.145812  1.830040\n",
            "4 -0.554816  1.108784 -0.297039  2.102812  1.934839\n",
            "          X0        X1        X2        X3         Y\n",
            "95  0.867404  0.809470 -0.698813 -0.211407  1.570258\n",
            "96 -0.314125  0.650321 -0.194280  1.757960  1.591702\n",
            "97  0.582491 -0.352278 -0.710077 -0.552536  0.880668\n",
            "98  0.282594  0.018263 -0.221446  0.554800  1.030568\n",
            "99 -0.845601 -0.326285  0.720403  0.926539  1.558880\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean    -0.118678    0.071489   -0.024178    0.328351    1.095683\n",
            "std      0.967370    0.944765    0.942306    0.970823    0.758786\n",
            "min     -2.526303   -2.467023   -2.755133   -2.302795   -0.941971\n",
            "25%     -0.801464   -0.432237   -0.610036   -0.297111    0.607452\n",
            "50%     -0.116328    0.089406   -0.052193    0.308323    1.040780\n",
            "75%      0.514649    0.814424    0.518259    0.886281    1.561725\n",
            "max      2.377999    2.449439    2.124104    3.089323    3.298092\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0 -0.514139  0.237483  0.358881 -0.218946  0.836997\n",
            "1  1.155113  1.050713  1.344456  0.676361  2.679250\n",
            "2  0.423188  0.070643 -0.155115  0.343298  1.404289\n",
            "3  0.325719  0.770574  0.229862  0.145812  1.830040\n",
            "4 -0.554816  1.108784 -0.297039  2.102812  1.934839\n",
            "          X0        X1        X2        X3         Y\n",
            "95  0.867404  0.809470 -0.698813 -0.211407  1.570258\n",
            "96 -0.314125  0.650321 -0.194280  1.757960  1.591702\n",
            "97  0.582491 -0.352278 -0.710077 -0.552536  0.880668\n",
            "98  0.282594  0.018263 -0.221446  0.554800  1.030568\n",
            "99 -0.845601 -0.326285  0.720403  0.926539  1.558880\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean    -0.118678    0.071489   -0.024178    0.328351    1.095683\n",
            "std      0.967370    0.944765    0.942306    0.970823    0.758786\n",
            "min     -2.526303   -2.467023   -2.755133   -2.302795   -0.941971\n",
            "25%     -0.801464   -0.432237   -0.610036   -0.297111    0.607452\n",
            "50%     -0.116328    0.089406   -0.052193    0.308323    1.040780\n",
            "75%      0.514649    0.814424    0.518259    0.886281    1.561725\n",
            "max      2.377999    2.449439    2.124104    3.089323    3.298092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0pQVHsrKLAB",
        "colab_type": "text"
      },
      "source": [
        "Problem 3: a)Linear regression using gradient descent                         \n",
        "                       b)Logistic regression using gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuvSd0F2Yq6W",
        "colab_type": "code",
        "outputId": "5b4c28da-4785-4adb-c594-4b4cb291e340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "#linear regression using the gradient descent\n",
        "print(\" linear regrission using gradient descent are\")\n",
        "X = df.iloc[:,0].values\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2\n",
        "  d1 = (-2/n) * sum(X * (y - y_p))\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "print(b1,b0)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#logistic regression using gradient descent\n",
        "print(\"from logistic using gradient descent are\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat)))\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz)\n",
        "  db = np.sum(dz)\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " linear regrission using gradient descent are\n",
            "0.03216582881154568 0.19915001949145356\n",
            "\n",
            "\n",
            "\n",
            "from logistic using gradient descent are\n",
            "0.6931471805599453\n",
            "0.38617574626623036\n",
            "0.3859692334552483\n",
            "0.38576709975182466\n",
            "0.3855692778858629\n",
            "0.3853757010083668\n",
            "0.38518630267511916\n",
            "0.38500101683313376\n",
            "0.3848197778097992\n",
            "0.3846425203046256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu4oNn-oLQHZ",
        "colab_type": "text"
      },
      "source": [
        "c)Linear Regression using L1 and L2 regularization                              \n",
        "d)Logistic regression using L1 and L2 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL2fLbxsYvVY",
        "colab_type": "code",
        "outputId": "55521f9f-0454-4765-8998-959fc8d3026b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        }
      },
      "source": [
        "print(\"Linear regression using L1 regularisation\")\n",
        "X = df.iloc[:,0].values\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + (lam * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + lam\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "print(b1,b0)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Linear regression using L2 regularisation\")\n",
        "X = df.iloc[:,0].values\n",
        "#print(X)\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + ((lam/2) * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + (lam *b1)\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "print(b1,b0)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Logistic regression using L1 regularisation\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(W)))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)\n",
        "\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Logistic regression using L2 regularisation\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(np.square(W))))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam * W\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear regression using L1 regularisation\n",
            "0.023041465124746533 0.19904641793682673\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Linear regression using L2 regularisation\n",
            "0.03201542386415001 0.19914888628676303\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Logistic regression using L1 regularisation\n",
            "0.6931471805599453\n",
            "-0.01142288085404708\n",
            "-0.4051398829468036\n",
            "-0.7945655137154224\n",
            "-1.179745937878965\n",
            "-1.5607280099319878\n",
            "-1.9375591986009284\n",
            "-2.310287514157183\n",
            "-2.678961438494434\n",
            "-3.0436298578829724\n",
            "\n",
            "\n",
            "\n",
            "Logistic regression using L2 regularisation\n",
            "0.6931471805599453\n",
            "0.3863856829342244\n",
            "0.386791899616714\n",
            "0.3875805094118645\n",
            "0.38872774094482165\n",
            "0.39021086657851783\n",
            "0.39200816539236544\n",
            "0.3940988873155417\n",
            "0.39646321836374504\n",
            "0.39908224693275574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6FqHJruLrjV",
        "colab_type": "text"
      },
      "source": [
        "e)K-means Clustering "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RRkhDBZY7MY",
        "colab_type": "code",
        "outputId": "af31a205-998d-4c88-80ae-ea4bcd1d57b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]\n",
        "X = df3.iloc[:,0:2].values\n",
        "clf = K_Means()\n",
        "clf.fit(X)\n",
        "\n",
        "for centroid in clf.centroids:\n",
        "    plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1],\n",
        "                marker=\"o\", color=\"k\", s=150, linewidths=5)\n",
        "\n",
        "for classification in clf.classifications:\n",
        "    color = colors[classification]\n",
        "    for featureset in clf.classifications[classification]:\n",
        "        plt.scatter(featureset[0], featureset[1], marker=\"x\", color=color, s=150, linewidths=5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaBVxF6RL4CU",
        "colab_type": "text"
      },
      "source": [
        "Problem 4 :Linear Regression from scratch using OOPS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MBIqfDuZAKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "class LinearRegressionModel():\n",
        "\n",
        "    def __init__(self, dataset, learning_rate, num_iterations):\n",
        "        self.dataset = np.array(dataset)\n",
        "        self.b = 0  \n",
        "        self.m = 0  \n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.M = len(self.dataset)\n",
        "        self.total_error = 0\n",
        "\n",
        "    def apply_gradient_descent(self):\n",
        "        for i in range(self.num_iterations):\n",
        "            self.do_gradient_step()\n",
        "\n",
        "    def do_gradient_step(self):\n",
        "        b_summation = 0\n",
        "        m_summation = 0\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            b_summation += (((self.m * x_value) + self.b) - y_value) \n",
        "            m_summation += (((self.m * x_value) + self.b) - y_value) * x_value\n",
        "        self.b = self.b - (self.learning_rate * (1/self.M) * b_summation)\n",
        "        self.m = self.m - (self.learning_rate * (1/self.M) * m_summation)\n",
        "      \n",
        "    def compute_error(self):\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            self.total_error += ((self.m * x_value) + self.b) - y_value\n",
        "        return self.total_error\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Results: b: {}, m: {}, Final Total error: {}\".format(round(self.b, 2), round(self.m, 2), round(self.compute_error(), 2))\n",
        "\n",
        "    def get_prediction_based_on(self, x):\n",
        "        return round(float((self.m * x) + self.b), 2) # Type: Numpy float.\n",
        "\n",
        "def main():\n",
        "    school_dataset = np.genfromtxt(DATASET_PATH, delimiter=\",\")\n",
        "    lr = LinearRegressionModel(school_dataset, 0.0001, 1000)\n",
        "    lr.apply_gradient_descent()\n",
        "    hours = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "    for hour in hours:\n",
        "        print(\"Studied {} hours and got {} points.\".format(hour, lr.get_prediction_based_on(hour)))\n",
        "    print(lr)\n",
        "\n",
        "if __name__ == \"__main__\": main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2AiZS-eMFRR",
        "colab_type": "text"
      },
      "source": [
        "Logistic Regression using oops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37AURcbRZBcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self, learning_rate, num_iters, fit_intercept = True, verbose = False):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iters = num_iters\n",
        "    self.fit_intercept = fit_intercept\n",
        "    self.verbose = verbose\n",
        "  def __add_intercept(self, X):\n",
        "    intercept = np.ones((X.shape[0],1))\n",
        "    return np.concatenate((intercept,X),axis=1)\n",
        "  def __sigmoid(self,z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "  def __loss(self, h, y):\n",
        "    return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()\n",
        "  \n",
        "  def fit(self,X,y):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    self.theta = np.zeros(X.shape[1])\n",
        "    \n",
        "    for i in range(self.num_iters):\n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      gradient = np.dot(X.T,(h-y))/y.size\n",
        "      \n",
        "      self.theta -= self.learning_rate * gradient\n",
        "      \n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      loss = self.__loss(h,y)\n",
        "      \n",
        "      if self.verbose == True and i % 1000 == 0:\n",
        "        print(f'Loss: {loss}\\t')\n",
        "  def predict_probability(self,X):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    return self.__sigmoid(np.dot(X,self.theta))\n",
        "  def predict(self,X):\n",
        "    return (self.predict_probability(X).round())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlW7KyNOMAcy",
        "colab_type": "text"
      },
      "source": [
        "K means from scratch using oops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODgvm-8nZFVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}